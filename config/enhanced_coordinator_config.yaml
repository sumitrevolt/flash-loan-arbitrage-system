# Enhanced LangChain Coordinator Configuration
# ==========================================

# MCP Server Configuration
mcp_servers:
  context7-mcp:
    port: 8001
    capabilities: ['context', 'search', 'retrieval']
    health_endpoint: '/health'
    execute_endpoint: '/execute'
    priority: 1
    max_retries: 3
    timeout: 30
    
  enhanced-copilot-mcp:
    port: 8002
    capabilities: ['code', 'generation', 'analysis']
    health_endpoint: '/health'
    execute_endpoint: '/execute'
    priority: 1
    max_retries: 3
    timeout: 30
    
  blockchain-mcp:
    port: 8003
    capabilities: ['blockchain', 'web3', 'contracts']
    health_endpoint: '/health'
    execute_endpoint: '/execute'
    priority: 2
    max_retries: 5
    timeout: 45
    
  price-oracle-mcp:
    port: 8004
    capabilities: ['prices', 'market_data', 'analytics']
    health_endpoint: '/health'
    execute_endpoint: '/execute'
    priority: 2
    max_retries: 3
    timeout: 20
    
  dex-services-mcp:
    port: 8005
    capabilities: ['dex', 'trading', 'liquidity']
    health_endpoint: '/health'
    execute_endpoint: '/execute'
    priority: 1
    max_retries: 3
    timeout: 30
    
  flash-loan-mcp:
    port: 8006
    capabilities: ['flash_loans', 'arbitrage', 'defi']
    health_endpoint: '/health'
    execute_endpoint: '/execute'
    priority: 1
    max_retries: 5
    timeout: 60

# LLM Configuration
llm:
  model: 'llama2'
  temperature: 0.7
  max_tokens: 2048
  top_p: 0.9
  frequency_penalty: 0.0
  presence_penalty: 0.0
  
# Alternative models (uncomment to use)
# llm:
#   provider: 'openai'
#   model: 'gpt-4'
#   api_key: 'your-api-key'
#   temperature: 0.7

# Memory Configuration
memory:
  type: 'summary_buffer'  # Options: 'buffer', 'summary', 'summary_buffer', 'entity'
  max_token_limit: 2000
  return_messages: true
  buffer_size: 100
  summary_frequency: 50

# Embeddings Configuration
embeddings:
  model: 'sentence-transformers/all-MiniLM-L6-v2'
  # Alternative: 'sentence-transformers/all-mpnet-base-v2'
  cache_folder: './embeddings_cache'
  device: 'cpu'  # or 'cuda' if available

# Vector Store Configuration
vectorstore:
  type: 'chroma'
  persist_directory: './chroma_db'
  collection_name: 'mcp_knowledge_base'
  distance_metric: 'cosine'
  max_documents: 10000

# Monitoring Configuration
monitoring:
  health_check_interval: 30  # seconds
  metrics_collection_interval: 60  # seconds
  performance_optimization_interval: 300  # seconds
  auto_restart_on_failure: true
  max_restart_attempts: 3
  restart_cooldown: 60  # seconds
  
  # Thresholds
  thresholds:
    cpu_usage_warning: 70
    cpu_usage_critical: 90
    memory_usage_warning: 80
    memory_usage_critical: 95
    response_time_warning: 5.0  # seconds
    response_time_critical: 10.0  # seconds
    error_rate_warning: 0.05  # 5%
    error_rate_critical: 0.10  # 10%

# Agent Configuration
agents:
  coordinator:
    type: 'react'
    max_iterations: 10
    early_stopping_method: 'generate'
    verbose: true
    
  analyzer:
    type: 'structured_chat'
    max_iterations: 5
    memory_key: 'chat_history'
    
  executor:
    type: 'openai_functions'
    max_iterations: 15
    return_intermediate_steps: true

# Chain Configuration
chains:
  health_analysis:
    type: 'llm'
    output_parser: 'json'
    
  mcp_coordination:
    type: 'sequential'
    memory_enabled: true
    
  performance_optimization:
    type: 'transform'
    async_mode: true

# Database Configuration
database:
  url: 'postgresql://postgres:password@localhost:5432/langchain_mcp'
  pool_size: 10
  max_overflow: 20
  pool_timeout: 30
  pool_recycle: 3600

# Redis Configuration
redis:
  host: 'localhost'
  port: 6379
  db: 0
  password: null
  socket_timeout: 5
  connection_pool:
    max_connections: 50

# Docker Configuration
docker:
  auto_cleanup: true
  restart_policy: 'unless-stopped'
  network_name: 'langchain_network'
  log_driver: 'json-file'
  log_options:
    max-size: '10m'
    max-file: '3'

# Logging Configuration
logging:
  level: 'INFO'  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: 'enhanced_langchain_coordinator.log'
  max_file_size: 10485760  # 10MB
  backup_count: 5
  colored_output: true

# Security Configuration
security:
  api_keys:
    enabled: false
    required_for_endpoints: []
  
  cors:
    enabled: true
    allow_origins: ['*']
    allow_methods: ['GET', 'POST', 'PUT', 'DELETE']
  
  rate_limiting:
    enabled: true
    requests_per_minute: 100
    burst_size: 20

# Advanced Features
advanced:
  # Auto-scaling
  auto_scaling:
    enabled: true
    min_instances: 1
    max_instances: 5
    scale_up_threshold: 80  # CPU %
    scale_down_threshold: 30  # CPU %
    cooldown_period: 300  # seconds
  
  # Load balancing
  load_balancing:
    enabled: true
    algorithm: 'round_robin'  # round_robin, least_connections, weighted
    health_check_enabled: true
  
  # Caching
  caching:
    enabled: true
    ttl: 3600  # seconds
    max_size: 1000  # number of entries
    
  # Circuit breaker
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    recovery_timeout: 60  # seconds
    half_open_max_calls: 3

# Feature Flags
features:
  experimental_chains: false
  advanced_retrieval: true
  multi_modal_support: false
  real_time_streaming: true
  auto_documentation: true
  performance_profiling: true
