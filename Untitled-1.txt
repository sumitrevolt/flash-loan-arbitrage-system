#!/usr/bin/env python3
"""
Ultimate LangChain MCP Project Organizer & Cleanup Commander
=========================================================

This advanced system uses LangChain to command all MCP servers and AI agents
to organize the project, remove unnecessary files, and create a clean structure.

Features:
- LangChain-powered intelligent file analysis
- Multi-agent coordination for cleanup tasks
- MCP server orchestration for project organization
- Automated duplicate detection and removal
- Smart file categorization and archiving
- Configuration consolidation
- Documentation generation

Author: GitHub Copilot Assistant
Date: June 18, 2025
"""

import asyncio
import json
import logging
import os
import shutil
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple
from collections import defaultdict
import requests
import glob
import re

# LangChain imports
try:
    from langchain.agents import AgentExecutor, create_react_agent
    from langchain.tools import Tool, BaseTool
    from langchain.prompts import PromptTemplate
    from langchain.memory import ConversationBufferWindowMemory
    from langchain.schema import SystemMessage, HumanMessage, AIMessage
    from langchain_openai import ChatOpenAI
    from langchain.agents.agent_types import AgentType
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("⚠️  LangChain not available, using fallback mode")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('langchain_project_organization.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LangChainMCPOrganizer:
    """Ultimate project organizer using LangChain and MCP servers"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root or os.getcwd())
        self.duplicates_found: Dict[str, List[Path]] = {}
        self.unnecessary_files: List[Path] = []
        self.mcp_servers_active: Dict[str, bool] = {}
        self.organization_stats = {
            'files_processed': 0,
            'duplicates_removed': 0,
            'directories_created': 0,
            'files_archived': 0,
            'space_saved': 0
        }
        
        # Initialize LangChain components
        self.llm = None
        self.agent_executor = None
        self.memory = None
        self.setup_langchain()
        
        # Define clean project structure
        self.target_structure = {
            'src': {
                'core': ['Main application logic'],
                'contracts': ['Smart contracts'],
                'mcp_servers': ['MCP server implementations'],
                'ai_agents': ['AI agent implementations'],
                'tools': ['Utility tools and helpers'],
                'integrations': ['External service integrations']
            },
            'config': ['Configuration files'],
            'docs': ['Documentation'],
            'scripts': ['Deployment and utility scripts'],
            'tests': ['Unit and integration tests'],
            'data': ['Data files and databases'],
            'logs': ['Log files'],
            'archive': {
                'duplicates': ['Archived duplicate files'],
                'backups': ['Project backups'],
                'deprecated': ['Deprecated code']
            }
        }
        
        # File patterns to remove
        self.cleanup_patterns = [
            '*.log',
            '*.tmp',
            '*temp*',
            '*backup*',
            '*duplicate*',
            '*old*',
            '*_old.*',
            '*_backup.*',
            '*_copy.*',
            '*_fixed.*',
            '*test_*',
            '*.pyc',
            '__pycache__',
            '.DS_Store',
            'Thumbs.db',
            '*.bak'
        ]
        
        # Directories to clean/consolidate
        self.consolidate_dirs = [
            'organized',
            'organized_project',
            'cleanup_backup',
            'backup_duplicates',
            'containers',
            'docker-configs',
            'enhanced_system_env'
        ]

    def setup_langchain(self):
        """Setup LangChain components for intelligent organization"""
        if not LANGCHAIN_AVAILABLE:
            logger.warning("LangChain not available, using basic organization")
            return
        
        try:
            # Initialize OpenAI LLM
            self.llm = ChatOpenAI(
                model="gpt-4",
                temperature=0.1,
                max_tokens=2000
            )
            
            # Setup memory
            self.memory = ConversationBufferWindowMemory(
                k=10,
                memory_key="chat_history",
                return_messages=True
            )
            
            # Create tools for LangChain agent
            tools = [
                Tool(
                    name="detect_duplicates",
                    description="Detect duplicate files in the project",
                    func=self._detect_duplicates_tool
                ),
                Tool(
                    name="analyze_file_importance",
                    description="Analyze file importance and categorize for organization",
                    func=self._analyze_file_importance_tool
                ),
                Tool(
                    name="organize_mcp_servers",
                    description="Organize and command MCP servers",
                    func=self._organize_mcp_servers_tool
                ),
                Tool(
                    name="cleanup_directory",
                    description="Clean up and organize directory structure",
                    func=self._cleanup_directory_tool
                )
            ]
            
            # Create prompt template
            prompt = PromptTemplate(
                input_variables=["tools", "tool_names", "input", "agent_scratchpad", "chat_history"],
                template="""
You are an expert project organizer and cleanup specialist. Your task is to analyze 
the flash loan arbitrage project and organize it efficiently using available tools.

Available tools: {tool_names}
{tools}

Current conversation:
{chat_history}

Task: {input}

Reasoning process:
{agent_scratchpad}

Please organize the project systematically, removing duplicates and unnecessary files.
"""
            )
            
            # Create agent
            agent = create_react_agent(self.llm, tools, prompt)
            self.agent_executor = AgentExecutor.from_agent_and_tools(
                agent=agent,
                tools=tools,
                memory=self.memory,
                verbose=True,
                handle_parsing_errors=True
            )
            
            logger.info("✅ LangChain components initialized successfully")
            
        except Exception as e:
            logger.error(f"❌ Error setting up LangChain: {e}")
            self.llm = None

    def _detect_duplicates_tool(self, input_str: str) -> str:
        """Tool function for duplicate detection"""
        try:
            duplicates = self.scan_for_duplicates()
            return f"Found {len(duplicates)} duplicate file groups with {sum(len(files) for files in duplicates.values())} total duplicates"
        except Exception as e:
            return f"Error detecting duplicates: {e}"

    def _analyze_file_importance_tool(self, file_path: str) -> str:
        """Tool function for file importance analysis"""
        try:
            path = Path(file_path)
            if not path.exists():
                return f"File {file_path} does not exist"
            
            importance = self.analyze_file_importance(path)
            return f"File importance: {importance['category']} - {importance['reason']}"
        except Exception as e:
            return f"Error analyzing file: {e}"

    def _organize_mcp_servers_tool(self, input_str: str) -> str:
        """Tool function for MCP server organization"""
        try:
            result = self.command_all_mcp_servers()
            return f"MCP servers organized: {len(result)} servers processed"
        except Exception as e:
            return f"Error organizing MCP servers: {e}"

    def _cleanup_directory_tool(self, directory: str) -> str:
        """Tool function for directory cleanup"""
        try:
            dir_path = Path(directory)
            if dir_path.exists():
                cleaned = self.cleanup_directory(dir_path)
                return f"Cleaned directory {directory}: {cleaned} files processed"
            return f"Directory {directory} does not exist"
        except Exception as e:
            return f"Error cleaning directory: {e}"

    async def run_intelligent_organization(self) -> Dict[str, Any]:
        """Run LangChain-powered intelligent project organization"""
        logger.info("🚀 Starting LangChain-powered project organization...")
        
        if self.agent_executor:
            try:
                # Use LangChain agent for intelligent organization
                result = await self.agent_executor.arun(
                    input=f"""
                    Please organize the flash loan arbitrage project at {self.project_root}.
                    
                    Tasks to complete:
                    1. Detect and remove duplicate files
                    2. Organize MCP servers and AI agents
                    3. Clean up unnecessary files and directories
                    4. Create proper project structure
                    5. Consolidate configuration files
                    
                    The project currently has many duplicate organizer files, temporary files,
                    and scattered MCP servers that need to be consolidated.
                    """
                )
                logger.info(f"🤖 LangChain agent result: {result}")
            except Exception as e:
                logger.error(f"❌ LangChain agent error: {e}")
        
        # Run fallback organization regardless
        return await self.run_comprehensive_cleanup()

    async def run_comprehensive_cleanup(self) -> Dict[str, Any]:
        """Run comprehensive cleanup without LangChain dependency"""
        logger.info("🧹 Starting comprehensive project cleanup...")
        
        tasks = [
            ("Scanning for duplicates", self.scan_for_duplicates),
            ("Detecting unnecessary files", self.detect_unnecessary_files),
            ("Creating target structure", self.create_target_structure),
            ("Commanding MCP servers", self.command_all_mcp_servers),
            ("Removing duplicates", self.remove_duplicates),
            ("Archiving unnecessary files", self.archive_unnecessary_files),
            ("Consolidating configurations", self.consolidate_configurations),
            ("Organizing remaining files", self.organize_remaining_files),
            ("Cleaning empty directories", self.clean_empty_directories),
            ("Generating final report", self.generate_organization_report)
        ]
        
        for task_name, task_func in tasks:
            try:
                logger.info(f"📋 {task_name}...")
                if asyncio.iscoroutinefunction(task_func):
                    result = await task_func()
                else:
                    result = task_func()
                logger.info(f"✅ {task_name} completed")
            except Exception as e:
                logger.error(f"❌ Error in {task_name}: {e}")
        
        return self.organization_stats

    def scan_for_duplicates(self) -> Dict[str, List[Path]]:
        """Scan for duplicate files using content hashing"""
        logger.info("🔍 Scanning for duplicate files...")
        
        file_hashes: Dict[str, List[Path]] = defaultdict(list)
        
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file() and not self._should_skip_file(file_path):
                try:
                    file_hash = self._get_file_hash(file_path)
                    file_hashes[file_hash].append(file_path)
                    self.organization_stats['files_processed'] += 1
                except Exception as e:
                    logger.warning(f"⚠️ Could not hash {file_path}: {e}")
        
        # Find actual duplicates (files with same hash)
        duplicates = {
            hash_val: paths for hash_val, paths in file_hashes.items()
            if len(paths) > 1
        }
        
        self.duplicates_found = duplicates
        logger.info(f"🔍 Found {len(duplicates)} duplicate file groups")
        
        return duplicates

    def detect_unnecessary_files(self) -> List[Path]:
        """Detect unnecessary files that should be archived or removed"""
        logger.info("🗑️ Detecting unnecessary files...")
        
        unnecessary = []
        
        for pattern in self.cleanup_patterns:
            matches = list(self.project_root.rglob(pattern))
            unnecessary.extend(matches)
        
        # Additional unnecessary file detection
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file():
                # Check for test files, backup files, etc.
                if any(keyword in str(file_path).lower() for keyword in 
                       ['test_', 'backup', 'copy', 'duplicate', 'temp', 'old']):
                    if file_path not in unnecessary:
                        unnecessary.append(file_path)
        
        self.unnecessary_files = unnecessary
        logger.info(f"🗑️ Found {len(unnecessary)} unnecessary files")
        
        return unnecessary

    def create_target_structure(self) -> None:
        """Create the target directory structure"""
        logger.info("📁 Creating target directory structure...")
        
        def create_dirs(structure: Dict, parent: Path):
            for name, content in structure.items():
                dir_path = parent / name
                dir_path.mkdir(exist_ok=True, parents=True)
                self.organization_stats['directories_created'] += 1
                
                if isinstance(content, dict):
                    create_dirs(content, dir_path)
                elif isinstance(content, list):
                    # Create README for the directory
                    readme_path = dir_path / "README.md"
                    if not readme_path.exists():
                        with open(readme_path, 'w') as f:
                            f.write(f"# {name.title()}\n\n")
                            f.write(f"This directory contains: {', '.join(content)}\n")
        
        create_dirs(self.target_structure, self.project_root)
        logger.info("✅ Target directory structure created")

    def command_all_mcp_servers(self) -> Dict[str, Any]:
        """Command all MCP servers to participate in organization"""
        logger.info("🤖 Commanding all MCP servers for organization...")
        
        # Find all MCP server files
        mcp_files = []
        for pattern in ['*mcp*server*.py', '*mcp*.py']:
            mcp_files.extend(self.project_root.rglob(pattern))
        
        # Remove duplicates and organize MCP servers
        mcp_servers_dir = self.project_root / 'src' / 'mcp_servers'
        mcp_servers_dir.mkdir(exist_ok=True, parents=True)
        
        organized_servers = {}
        server_configs = {}
        
        for mcp_file in mcp_files:
            if 'archive' in str(mcp_file) or 'backup' in str(mcp_file):
                continue
            
            server_name = mcp_file.stem
            target_path = mcp_servers_dir / mcp_file.name
            
            # Move to organized location if not already there
            if mcp_file.parent != mcp_servers_dir:
                if not target_path.exists():
                    shutil.move(str(mcp_file), str(target_path))
                    logger.info(f"📦 Moved {server_name} to organized location")
            
            organized_servers[server_name] = str(target_path)
            server_configs[server_name] = {
                "path": f"src/mcp_servers/{mcp_file.name}",
                "type": "python",
                "port": 8000 + len(server_configs),
                "enabled": True,
                "organized": True
            }
        
        # Save unified MCP configuration
        config_path = self.project_root / 'config' / 'mcp_servers.json'
        config_path.parent.mkdir(exist_ok=True)
        with open(config_path, 'w') as f:
            json.dump({
                "servers": server_configs,
                "organization_date": datetime.now().isoformat(),
                "total_servers": len(server_configs)
            }, f, indent=2)
        
        logger.info(f"🤖 Organized {len(organized_servers)} MCP servers")
        return organized_servers

    def remove_duplicates(self) -> None:
        """Remove duplicate files intelligently"""
        logger.info("🗑️ Removing duplicate files...")
        
        for file_hash, duplicate_files in self.duplicates_found.items():
            if len(duplicate_files) <= 1:
                continue
            
            # Sort by preference (keep the one in the best location)
            best_file = self._choose_best_duplicate(duplicate_files)
            files_to_remove = [f for f in duplicate_files if f != best_file]
            
            for file_to_remove in files_to_remove:
                try:
                    # Move to archive instead of deleting
                    archive_dir = self.project_root / 'archive' / 'duplicates'
                    archive_dir.mkdir(exist_ok=True, parents=True)
                    
                    archive_path = archive_dir / file_to_remove.name
                    # Handle name conflicts in archive
                    counter = 1
                    while archive_path.exists():
                        stem = file_to_remove.stem
                        suffix = file_to_remove.suffix
                        archive_path = archive_dir / f"{stem}_duplicate_{counter}{suffix}"
                        counter += 1
                    
                    shutil.move(str(file_to_remove), str(archive_path))
                    self.organization_stats['duplicates_removed'] += 1
                    logger.info(f"📦 Archived duplicate: {file_to_remove.name}")
                    
                except Exception as e:
                    logger.error(f"❌ Error removing duplicate {file_to_remove}: {e}")

    def archive_unnecessary_files(self) -> None:
        """Archive unnecessary files"""
        logger.info("📦 Archiving unnecessary files...")
        
        for file_path in self.unnecessary_files:
            if not file_path.exists():
                continue
            
            try:
                # Determine archive category
                if 'test' in str(file_path).lower():
                    archive_subdir = 'tests'
                elif any(term in str(file_path).lower() for term in ['backup', 'old']):
                    archive_subdir = 'backups'
                else:
                    archive_subdir = 'deprecated'
                
                archive_dir = self.project_root / 'archive' / archive_subdir
                archive_dir.mkdir(exist_ok=True, parents=True)
                
                archive_path = archive_dir / file_path.name
                # Handle name conflicts
                counter = 1
                while archive_path.exists():
                    stem = file_path.stem
                    suffix = file_path.suffix
                    archive_path = archive_dir / f"{stem}_archived_{counter}{suffix}"
                    counter += 1
                
                shutil.move(str(file_path), str(archive_path))
                self.organization_stats['files_archived'] += 1
                logger.info(f"📦 Archived: {file_path.name}")
                
            except Exception as e:
                logger.error(f"❌ Error archiving {file_path}: {e}")

    def consolidate_configurations(self) -> None:
        """Consolidate configuration files"""
        logger.info("⚙️ Consolidating configuration files...")
        
        config_dir = self.project_root / 'config'
        config_dir.mkdir(exist_ok=True)
        
        # Find and consolidate config files
        config_files = []
        for pattern in ['*config*', '*.json', '*.yaml', '*.yml', '*.ini', '.env*']:
            config_files.extend(self.project_root.glob(pattern))
        
        consolidated_configs = {}
        
        for config_file in config_files:
            if config_file.is_file() and 'node_modules' not in str(config_file):
                target_path = config_dir / config_file.name
                
                if config_file.parent != config_dir:
                    try:
                        if not target_path.exists():
                            shutil.move(str(config_file), str(target_path))
                            logger.info(f"⚙️ Moved config: {config_file.name}")
                        consolidated_configs[config_file.name] = str(target_path)
                    except Exception as e:
                        logger.error(f"❌ Error moving config {config_file}: {e}")
        
        # Create master configuration index
        master_config = self.project_root / 'config' / 'master_config.json'
        with open(master_config, 'w') as f:
            json.dump({
                "configurations": consolidated_configs,
                "consolidation_date": datetime.now().isoformat(),
                "project_root": str(self.project_root)
            }, f, indent=2)

    def organize_remaining_files(self) -> None:
        """Organize remaining files into proper structure"""
        logger.info("📁 Organizing remaining files...")
        
        file_mappings = {
            '.py': 'src',
            '.js': 'scripts',
            '.ts': 'scripts',
            '.sol': 'src/contracts',
            '.md': 'docs',
            '.txt': 'docs',
            '.json': 'config',
            '.yaml': 'config',
            '.yml': 'config'
        }
        
        for file_path in self.project_root.rglob("*"):
            if (file_path.is_file() and 
                not self._is_in_organized_structure(file_path) and
                not self._should_skip_file(file_path)):
                
                suffix = file_path.suffix.lower()
                target_dir = file_mappings.get(suffix)
                
                if target_dir:
                    target_path = self.project_root / target_dir
                    target_path.mkdir(exist_ok=True, parents=True)
                    
                    new_path = target_path / file_path.name
                    
                    # Avoid conflicts
                    if not new_path.exists():
                        try:
                            shutil.move(str(file_path), str(new_path))
                            logger.info(f"📁 Organized: {file_path.name} -> {target_dir}")
                        except Exception as e:
                            logger.error(f"❌ Error organizing {file_path}: {e}")

    def clean_empty_directories(self) -> None:
        """Clean up empty directories"""
        logger.info("🧹 Cleaning empty directories...")
        
        def is_empty_dir(path: Path) -> bool:
            return path.is_dir() and not any(path.rglob("